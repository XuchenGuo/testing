# df-analyze — Adaptive Error Rate (aER) + Multi‑Target Learning Integration

This README documents two major integrations in **df-analyze**:

1. **Adaptive Error Rate (aER)**: a calibration + reliability analysis layer that produces *per-sample expected error estimates*, *risk–coverage curves*, and *risk-controlled thresholds* for selective prediction.
2. **Multi-target learning**: first-class support for **multiple target columns** in a single run, including multi-target implementations of **KNN**, **CatBoost**, and **Dummy** models.

This document is intentionally focused on these integrations. The baseline df-analyze pipeline (data ingestion, inspection, cleaning, standard univariate reports, standard feature selection outputs, etc.) is assumed to be documented elsewhere in your project’s existing README. The sections below explain what changed, why it is scientifically grounded, and how to interpret the new output artifacts.

---

## Contents

- [1. Concepts and terminology](#1-concepts-and-terminology)
- [2. Quickstart: enabling aER and multi-target](#2-quickstart-enabling-aer-and-multi-target)
- [3. Adaptive Error Rate (aER)](#3-adaptive-error-rate-aer)
  - [3.1 Research alignment](#31-research-alignment)
  - [3.2 How aER is integrated into the df-analyze pipeline](#32-how-aer-is-integrated-into-the-df-analyze-pipeline)
  - [3.3 Practical adaptations made for df-analyze](#33-practical-adaptations-made-for-df-analyze)
  - [3.4 aER configuration options](#34-aer-configuration-options)
  - [3.5 aER outputs: files and figures](#35-aer-outputs-files-and-figures)
- [4. Multi-target learning](#4-multi-target-learning)
  - [4.1 How multi-target is integrated into df-analyze](#41-how-multi-target-is-integrated-into-df-analyze)
  - [4.2 Multi-target feature selection aggregation](#42-multi-target-feature-selection-aggregation)
  - [4.3 Multi-target models](#43-multi-target-models)
- [5. Output changes in multi-target mode](#5-output-changes-in-multi-target-mode)
- [6. Developer map: where the integrations live](#6-developer-map-where-the-integrations-live)

---

## 1. Concepts and terminology

### Adaptive Error Rate (aER)

In standard classification, a model typically produces:

- a predicted label \(\hat{y}(x)\), and
- a “confidence-like” quantity, often derived from predicted probabilities.

**Adaptive Error Rate (aER)** turns this confidence signal into an *estimated probability of being wrong* on a **per-sample** basis:

- **aER(x)** ≈ “If the model gives a prediction like this, how likely is it that the prediction is incorrect?”

This is *not* the same as accuracy. Accuracy is a dataset-level average. aER is a sample-level estimate, designed for *reliability* and *selective prediction*.

### Confidence metric

A “confidence metric” is any scalar signal that monotonically tracks prediction certainty.

In the thesis framing, confidence is usually derived from model probabilities (binary or multiclass). In df-analyze, confidence can be:

- probability-based (e.g., probability margin),
- KNN-specific (e.g., vote fraction, neighbor-distance proxy),
- tree/ensemble-specific where applicable (kept for compatibility even if you don’t use those models).

df-analyze *selects* a confidence metric automatically unless you pin it explicitly.

### Out-of-fold (OOF) predictions

To estimate aER without cheating (leakage), df-analyze uses **out-of-fold predictions**:

- Split the training data into K folds.
- For each fold, train on K−1 folds and predict on the held-out fold.
- Concatenate the held-out predictions: every training sample has a prediction generated by a model that **did not train on that sample**.

OOF predictions act as the **calibration set** for learning the mapping from confidence → expected error.

### Risk–coverage curve (selective prediction)

Selective prediction allows the model to **abstain** on uncertain cases.

- **Coverage**: fraction of samples the model chooses to predict (not abstain).
- **Risk**: error rate among the predicted (non-abstained) samples.

A risk–coverage curve shows how accuracy improves as you become more selective, and supports **risk-controlled thresholds** (“predict only when aER ≤ t”).

### Multi-target learning (multi-output)

In this README, **multi-target** means:

- Your dataset has **multiple target columns** \(y_1, y_2, …, y_T\).
- df-analyze trains models that output predictions for all targets in one run.
- Feature selection is run per target and then *aggregated* to produce a shared feature subset (so the data flow stays consistent with the existing pipeline).

This is multi-output learning. It does **not** assume or enforce target dependence; CatBoost and Dummy are implemented as **one model per target** under a single df-analyze model wrapper, while KNN can natively fit multi-output targets.

---

## 2. Quickstart: enabling aER and multi-target

### 2.1 Enable aER (single-target classification)

```bash
python df-analyze.py \
  --df path/to/data.csv \
  --mode classify \
  --target outcome \
  --adaptive-error
```

Notes:

- aER only runs for **classification**.
- aER is computed **after** df-analyze finishes tuning/evaluation (it consumes tuned models and the train/test split already produced by the pipeline).

### 2.2 Enable multi-target learning

```bash
python df-analyze.py \
  --df path/to/data.csv \
  --mode classify \
  --targets outcome1,outcome2,outcome3 \
  --classifiers knn,catboost,dummy
```

Key constraints:

- All targets must be compatible with the chosen mode (all classification or all regression).
- **Multi-target tuning is currently restricted to**: **Dummy**, **KNN**, and **CatBoost** (because they are implemented with multi-output support in df-analyze).

### 2.3 Multi-target + aER together

aER is single-target by definition, but df-analyze runs it **per target** when multi-target mode is enabled.

```bash
python df-analyze.py \
  --df path/to/data.csv \
  --mode classify \
  --targets outcome1,outcome2 \
  --classifiers knn,catboost,dummy \
  --adaptive-error
```

This produces:

- the usual df-analyze outputs (with multi-target-aware differences),
- plus:
  - `adaptive_error/outcome1/...`
  - `adaptive_error/outcome2/...`

Each target gets a complete aER analysis directory.

How per-target aER is produced in multi-target runs:

- df-analyze reuses the *same* tuned multi-target run, then **slices** predictions/probabilities per target and feeds them into the existing single-target aER engine.
- For models that are implemented as “one estimator per target” (CatBoost/Dummy), the target-specific estimator is selected.
- For models that are natively multi-output (KNN), a lightweight wrapper slices the multi-output predictions down to a single target.

This keeps the aER implementation single-target (as in the thesis) while still supporting multi-target datasets without introducing a parallel analysis framework.

---

## 3. Adaptive Error Rate (aER)

### 3.1 Research alignment

df-analyze’s aER implementation is designed to be a **direct software realization** of the thesis methodology, while adapting to df-analyze’s existing pipeline constraints.

It explicitly aligns with:

- **Xuchen_Guo_MSc_Thesis** (definitions, assumptions, and methodology)
- **my_thesis_topic_oooo** (research motivation and intended usage: per-sample risk flags, clinician-facing outputs, and ensemble extensions)

In particular, df-analyze’s implementation is designed to mirror the thesis structure:

- **Thesis §2.1.2**: probability-derived confidence metrics (binary and multiclass).
- **Thesis §2.1.3**: aER definition \(\mathrm{aER}(x)\) and the confidence→error mapping \(h(c)\), estimated via binning and stabilized with a global-error prior.
- **Thesis §2.1.4–2.1.5**: selective prediction and **risk-controlled** threshold selection using conservative confidence bounds.
- **Thesis §2.6**: ensemble learning extensions that use risk estimates to choose or combine models.

The `clinician_view.*` and “high-risk case” outputs in df-analyze directly reflect the intended usage described in *my_thesis_topic_oooo* (communicating uncertainty to end users and flagging predictions that warrant review).

#### 3.1.1 What the thesis defines (conceptually)

The thesis defines a pipeline with these core ideas:

1. **Confidence** is computed from predicted probabilities (binary/multiclass cases).
2. We learn a mapping \(h(c)\) from confidence \(c\) to empirical error probability:
   - \(h(c) \approx \Pr(\hat{y}(x) \neq y(x) \mid c(x) = c)\)
3. The **Adaptive Error Rate** is:
   - \(\text{aER}(x) = h(c(x))\)
4. The mapping is estimated non-parametrically via **binning** of confidence values and empirical error rates, stabilized with **Bayesian shrinkage**.
5. The resulting risk estimate is used for:
   - reliability diagnostics (calibration of the risk estimate),
   - **risk–coverage** evaluation,
   - **risk-controlled thresholds** for selective prediction,
   - and optionally **ensemble strategies** that leverage per-sample risk.

#### 3.1.2 How df-analyze implements those same ideas

df-analyze mirrors these concepts with clear 1:1 correspondences:

| Thesis concept | df-analyze implementation |
|---|---|
| Confidence from probabilities | `analysis/adaptive_error/confidence_metrics.py` implements probability-derived confidence metrics (and some model-specific metrics). |
| Mapping \(h(c)\) via binned error rates | `analysis/adaptive_error/aer.py` (`AdaptiveErrorCalculator`) fits a confidence→expected-error mapping using bins. |
| Bayesian shrinkage with a global error prior | `AdaptiveErrorCalculator` supports a **prior strength** that pulls sparse bins toward the global error rate. |
| Calibration dataset separated from training | df-analyze uses **out-of-fold predictions** on the training split as a leakage-safe calibration proxy. |
| Risk–coverage / selective prediction | `analysis/adaptive_error/risk_control.py` and plotting in `analysis/adaptive_error/plots.py`. |
| Risk-controlled thresholds | `risk_control.find_threshold(...)` computes a threshold using an upper confidence bound criterion. |
| Ensemble extensions | `analysis/adaptive_error/ensemble_*` modules implement strategy families compatible with the thesis design. |

### 3.2 How aER is integrated into the df-analyze pipeline

aER is integrated as a **post-evaluation analysis stage**. It does not create a parallel training pipeline. It reuses the existing df-analyze flow and artifacts.

At a high level:

```
(df-analyze core pipeline)
Data → Prepare → (feature selection) → Tune models → Evaluate on holdout

(aER extension)
Use tuned models + train split:
  → build OOF predictions (calibration proxy)
  → choose a confidence metric
  → fit confidence → expected error mapping
  → evaluate mapping + risk metrics on holdout test
  → write risk/coverage outputs and clinician-facing tables
```

Below is the step-by-step integration in more detail.

#### Step 0 — Entry point and gating

- aER runs only if `--adaptive-error` is enabled.
- aER runs only for **classification**.

**Where this hooks in:** after `evaluate_tuned(...)` finishes, df-analyze calls the adaptive error runner (see `df_analyze/_main.py`).

#### Step 1 — Choose which tuned models to analyze

aER is computed for the **top-K tuned models** (by cross-validated score), controlled by:

- `--aer-top-k` (default: all available; set to e.g. 5 to limit runtime)

Implementation note:

- The **Dummy** baseline is automatically **skipped** for aER analysis (it is a non-informative baseline and does not provide meaningful uncertainty behavior).

This reuses the model ranking already produced by df-analyze’s tuning stage—no re-ranking logic is introduced.

#### Step 2 — Build out-of-fold (OOF) prediction tables (calibration proxy)

For each selected tuned model:

1. df-analyze reuses the existing training split \((X_{train}, y_{train})\).
2. It runs a K-fold OOF procedure (`--aer-oof-folds`) using df-analyze’s fold logic (same splitter abstraction, same grouping constraints if used).
3. It writes an **OOF per-sample table**, including:
   - the true label,
   - the out-of-fold predicted label,
   - the raw model probabilities/scores (when available),
   - candidate confidence metrics computed from those probabilities/scores.

This corresponds to the thesis requirement that the mapping be learned on data not used to train the prediction for the same sample.

#### Step 3 — Probability calibration (optional but preferred)

Confidence metrics are only meaningful when probabilities are well-behaved.

df-analyze therefore optionally applies an **external probability calibrator** (selected on OOF data) before computing confidence:

- possible calibrators include “none”, temperature scaling, Platt scaling, and isotonic variants (depending on feasibility and target structure),
- the selected method is recorded and reused consistently for both OOF and TEST computations.

This is a practical, pipeline-friendly implementation of the thesis’s calibration assumptions.

#### Step 4 — Select the best confidence metric (automatic, reproducible)

Unless you specify `--aer-confidence-metric`, df-analyze selects a confidence metric automatically by:

1. evaluating candidate confidence metrics on OOF data,
2. measuring how well each metric predicts correctness (via a Brier-style criterion),
3. selecting the metric with the best reliability behavior.

This selection step is written to disk (so the run is reviewable and reproducible) and ensures the aER mapping is not tied to an arbitrary confidence definition.

#### Step 5 — Fit the confidence → expected error mapping

df-analyze fits the mapping \(h(c)\) using `AdaptiveErrorCalculator`:

1. **Bin** OOF confidence values into `--aer-bins` bins.
2. Compute **empirical error rate per bin**.
3. Apply **Bayesian shrinkage** toward the global OOF error rate with `--aer-prior-strength`.
4. Optionally:
   - smooth across adjacent bins (`--aer-smooth`),
   - enforce monotonicity (`--aer-monotonic`),
   - switch to quantile (“adaptive”) binning for skewed confidence distributions (`--aer-adaptive-binning`).

The result is a lookup function that converts any confidence value into an expected error estimate.

#### Step 6 — Evaluate on the holdout test set

Using the tuned model (trained on the full training split) and the held-out test split \((X_{test}, y_{test})\):

1. Predict labels and probabilities on the test set.
2. Apply the same probability calibrator selected in Step 3.
3. Compute the selected confidence metric.
4. Compute per-sample **aER** by applying the fitted mapping \(h(c)\).
5. Write:
   - per-sample predictions + risk estimates,
   - binned reliability summaries,
   - global diagnostic metrics (Brier/ECE-like error-of-error measures),
   - a confidence-vs-error plot for quick visual inspection.

#### Step 7 — Risk–coverage evaluation and risk-controlled threshold (best model)

For the best aER-analyzed model (by df-analyze’s tuning rank), df-analyze computes:

- the **risk–coverage curve** (coverage vs accuracy under selective prediction),
- a **risk-controlled threshold** \(t^*\) such that predicted cases satisfy a user-defined maximum error rate (with a conservative confidence bound):

  - target error: `--aer-target-error`
  - significance: `--aer-alpha`
  - minimum accepted predictions: `--aer-nmin`

This directly supports the thesis’s risk-controlled classification objective, while keeping the test set purely for reporting.

#### Step 8 — Cross-model comparison outputs

Finally, df-analyze writes cross-model summary tables and comparison plots so you can compare multiple tuned models’ aER behavior in one place.

This is especially useful when conventional metrics (accuracy/AUC) are similar but reliability differs.

#### Optional Step 9 — Ensemble analysis (if enabled)

If `--aer-ensemble` is enabled, df-analyze runs a suite of ensemble strategies that use per-sample risk estimates to choose or combine model predictions.

These strategies are implemented as an extension of the same aER artifacts:

- they reuse the OOF risk estimates,
- they produce the same kinds of outputs (per-sample risk, risk–coverage curves, thresholds),
- they live under a dedicated `adaptive_error/ensemble/` directory.

### 3.3 Practical adaptations made for df-analyze

The thesis describes an idealized experimental design with separate datasets (train / calibration / test).

df-analyze is designed as a general-purpose automated pipeline where:

- a single dataset is split into train/test,
- model selection uses CV on the train split.

To integrate aER *without introducing parallel data flows*, df-analyze makes these adaptations:

1. **Calibration proxy via OOF predictions**  
   Instead of requiring a dedicated calibration split, df-analyze uses **OOF predictions on the training split** as the calibration dataset.  
   This preserves the “prediction not trained on the same sample” requirement while keeping df-analyze’s single train/test split structure.

2. **Cross-fitted risk estimates for thresholding and ensembles**  
   When computing risk-controlled thresholds, df-analyze can compute **cross-fitted aER values** on OOF data (each sample’s risk estimated by a mapping fitted without that sample’s fold).  
   This reduces optimistic bias and matches the thesis’s emphasis on conservative reliability estimates.

3. **Robustness features for real datasets**  
   Real confidence distributions can be highly skewed and sparse near 1.0. To keep estimates stable:
   - df-analyze supports minimum bin counts (`--aer-min-bin-count`) and bin merging,
   - Bayesian shrinkage prevents “single-bin overconfidence” artifacts,
   - optional smoothing and monotonic constraints align the learned mapping with the theoretical assumption that higher confidence should not imply higher error.

4. **Per-target execution in multi-target mode**  
   aER is inherently single-target, but multi-target runs call the same single-target aER engine **once per target** by slicing existing multi-target predictions.  
   This avoids implementing a second, ad-hoc “multi-target aER” system.

### 3.4 aER configuration options

The aER integration is controlled via df-analyze CLI options. The most important ones are:

| Option | Meaning | Typical use |
|---|---|---|
| `--adaptive-error` | Enable aER analysis | Turn on aER outputs |
| `--aer-oof-folds K` | Number of folds for OOF calibration | 5 is a good default |
| `--aer-bins B` | Number of confidence bins | 10–30 depending on data size |
| `--aer-min-bin-count N` | Minimum samples per bin | Increase for small datasets |
| `--aer-prior-strength S` | Shrinkage strength toward global error | Increase when bins are sparse |
| `--aer-smooth` | Smooth expected error curve across bins | Use if curve is noisy |
| `--aer-monotonic` | Enforce monotonic decrease of error with confidence | Use if mapping violates theory due to noise |
| `--aer-adaptive-binning` | Use quantile bins instead of uniform bins | Helps when most confidences cluster near 1.0 |
| `--aer-confidence-metric NAME` | Fix confidence metric instead of auto-select | For controlled experiments |
| `--aer-target-error ε` | Target maximum error for selective prediction | e.g. 0.05 (5%) |
| `--aer-alpha α` | Significance for conservative error bound | e.g. 0.05 |
| `--aer-nmin N` | Minimum accepted predictions when choosing a threshold | Protects against trivial “accept 1 sample” thresholds |
| `--aer-top-k K` | Run aER only on top-K tuned models | Reduce runtime |
| `--aer-ensemble` | Enable ensemble strategies | Thesis-aligned ensemble evaluation |
| `--aer-ensemble-strategies ...` | Select which strategies to run | Controlled ablation studies |

### 3.5 aER outputs: files and figures

All aER artifacts live under:

- **single-target:** `adaptive_error/`
- **multi-target:** `adaptive_error/<target_name>/`

The aER output structure is designed to match df-analyze’s existing “pipeline stage directories” approach:

- predictable folder names,
- per-model subdirectories,
- machine-readable JSON/CSV artifacts plus human-readable Markdown summaries,
- placeholder “NOT_AVAILABLE” files when something fails (so downstream tooling does not break silently).

Below is a complete explanation of outputs generated when aER is enabled.

---

#### 3.5.1 Directory layout

```
adaptive_error/
  run_config.json
  plots/
    confidence_vs_expected_error_compare.png
    coverage_vs_accuracy_overlay.png              (only if --aer-ensemble)
  tables/
    models_ranked.csv
    aer_metrics_by_model.csv
    coverage_accuracy_overlay_summary.csv         (only if --aer-ensemble)
    coverage_accuracy_overlay_by_accuracy_summary.csv (only if --aer-ensemble)
  predictions/
    test_per_sample_multi_model.parquet
    test_per_sample_multi_model.csv
  models/
    <model_slug>/
      plots/
      tables/
      predictions/
      metadata/
      reports/
  ensemble/                                      (only if --aer-ensemble)
    tables/
    reports/
    <strategy_name>/
      plots/
      tables/
      predictions/
      metadata/
      reports/
```

---

#### 3.5.2 Root-level aER artifacts

These summarize *all* analyzed models for a single target.

##### `adaptive_error/run_config.json`

**What it is:** a machine-readable snapshot of the aER configuration used in this run.

**How it is generated:** written once at the start of aER execution.

**Why it matters:** ensures results are academically reviewable and reproducible. If two runs differ, this file tells you *exactly* what changed (number of bins, smoothing, threshold criteria, etc.).

---

##### `adaptive_error/tables/models_ranked.csv`

**What it is:** a compact index of the tuned models included in aER analysis.

**How it is generated:**

- df-analyze selects the top-K tuned results,
- assigns each one a stable `model_slug`,
- records the CV score used for ranking, and the relative output directory.

**Why it matters:** provides a quick “table of contents” for the per-model subdirectories, and ties the aER artifacts back to the tuning stage.

Key columns you will typically see:

- `rank`: ranking position according to df-analyze tuning
- `model_slug`: folder name under `adaptive_error/models/`
- `metric`: tuning metric used (e.g., accuracy, AUC)
- `cv_score`: CV score from tuning
- `test_accuracy`: accuracy on the holdout test split (for context)
- `n_features`: number of features used by the tuned model
- `out_dir_rel`: path to the per-model aER folder

---

##### `adaptive_error/tables/aer_metrics_by_model.csv`

**What it is:** a cross-model table of reliability metrics *for the aER estimates themselves*.

**How it is generated:**

- For each model, df-analyze computes:
  - `global_error_test`: observed test error rate
  - `brier_error_test`: mean squared error of the predicted error probabilities
  - `ece_error_test`: expected calibration error of the predicted error probabilities
- If a model’s aER stage failed, the row includes a “NOT_AVAILABLE” status and a reason.

**Why it matters:** accuracy alone does not tell you whether risk estimates are meaningful. This table provides a quantitative basis for comparing reliability across models.

---

##### `adaptive_error/plots/confidence_vs_expected_error_compare.png`

**What it is:** a comparison plot showing the learned mapping from confidence → expected error for each model.

**How it is generated:**

- Each model produces binned estimates of expected error as a function of confidence.
- These curves are overlaid to visualize differences across models.

**Why it matters:** two models can have similar accuracy but very different confidence behavior. This plot helps you detect models that are systematically overconfident or underconfident.

---

##### `adaptive_error/predictions/test_per_sample_multi_model.parquet` (+ `.csv`)

**What it is:** a per-sample comparison table across all analyzed models.

**How it is generated:**

- df-analyze aligns all models’ test predictions by row index (`row_id`),
- then stores each model’s:
  - predicted label,
  - confidence,
  - aER estimate.

**Why it matters:** this is the easiest way to answer sample-level questions like:

- “Which model is least risky on this patient?”
- “Do multiple models agree on the label but disagree on risk?”
- “Which samples are consistently high-risk across models?”

**Why both parquet and CSV:**

- Parquet preserves nested types and is better for programmatic analysis.
- CSV is human-readable and easy to inspect but may stringify complex fields.

---

##### `adaptive_error/plots/coverage_vs_accuracy_overlay.png` (only if `--aer-ensemble`)

**What it is:** an overlay plot comparing the best base model’s risk–coverage curve with ensemble strategies’ curves.

**How it is generated:** after ensemble analysis completes, df-analyze overlays curves for consistent visual comparison.

**Why it matters:** directly tests the thesis-motivated hypothesis that risk-aware ensembles can improve selective prediction behavior (higher accuracy at the same coverage, or higher coverage at the same accuracy).

---

#### 3.5.3 Per-model aER artifacts: `adaptive_error/models/<model_slug>/...`

Each analyzed model gets its own directory containing:

- metadata (JSON) for reproducibility and debugging,
- tables (CSV) for quantitative review,
- plots (PNG) for visual diagnostics,
- predictions (parquet/CSV) for per-sample auditing,
- short Markdown reports for human consumption.

Below is a file-by-file explanation, grouped by subdirectory.

---

### A) `.../metadata/`

##### `confidence_metric_selection.json`

**Represents:** which confidence metric df-analyze selected (or used), and how it compared against alternatives.

**Generated by:** the OOF stage, where candidate metrics are evaluated on OOF predictions.

**Purpose:** confidence choice is a *scientific degree of freedom*. This file makes it explicit and reviewable.

---

##### `proba_calibrator.json`

**Represents:** which probability calibration method was selected and its evaluation diagnostics.

**Generated by:** OOF stage calibration selection.

**Purpose:** confidence metrics depend on probability calibration. Without recording calibration, confidence-based reliability is not reproducible.

---

##### `confidence_to_expected_error_lookup.json`

**Represents:** the fitted mapping \(h(c)\) from confidence to expected error.

**Generated by:** the fit stage (`AdaptiveErrorCalculator`).

**Purpose:** this is the core artifact that defines aER for the model. It is required to:
- compute per-sample aER on new data,
- reproduce the curve without refitting.

---

##### `adaptive_error_metrics.json`

**Represents:** test-set summary metrics evaluating how good the aER estimates are as probability forecasts of error.

**Generated by:** test stage + writer.

**Purpose:** provides a quantitative basis for claims like “model A’s risk estimates are better calibrated than model B’s.”

---

##### `risk_control_threshold.json` (best model only)

**Represents:** the selected risk threshold \(t^*\) and coverage guarantees for risk-controlled prediction.

**Generated by:** risk-control stage on OOF risk estimates, using `--aer-target-error`, `--aer-alpha`, and `--aer-nmin`.

**Purpose:** provides an operational decision rule for selective prediction:
- accept predictions where aER ≤ t*,
- abstain otherwise,
with a conservative statistical guarantee.

---

##### `sanity_checks.json` (only when issues detected)

**Represents:** automatically-detected issues that might indicate broken confidence behavior, calibration drift, or implementation mismatch.

**Generated by:** test stage when certain heuristics trigger.

**Purpose:** helps reviewers or maintainers identify when an aER result should not be trusted blindly.

---

##### `error_traceback.txt` (only if a stage fails)

**Represents:** captured exception tracebacks during aER execution.

**Generated by:** error handling wrappers.

**Purpose:** turns silent failures into inspectable logs without crashing the whole df-analyze run.

---

### B) `.../tables/`

##### `confidence_to_expected_error_lookup.csv`

**Represents:** a tabular version of the mapping in `confidence_to_expected_error_lookup.json`.

**Generated by:** converting the fitted mapping to a DataFrame and saving to CSV.

**Purpose:** easy inspection and plotting in spreadsheets or notebooks.

---

##### `oof_confidence_error_bins.csv`

**Represents:** binned OOF statistics used to fit the mapping.

Typical contents include, per bin:

- confidence interval bounds (bin edges),
- number of samples in the bin,
- empirical error rate,
- shrunken/smoothed expected error estimate,
- uncertainty bounds (e.g., Wilson interval).

**Generated by:** `AdaptiveErrorCalculator.bin_stats_df(...)` on OOF data.

**Purpose:** shows whether the mapping is supported by sufficient data and whether certain confidence regions are poorly sampled.

---

##### `test_confidence_error_bins.csv`

**Represents:** the same type of binned table as above, but computed on the **test set**.

**Generated by:** applying the learned mapping to test confidences and binning.

**Purpose:** evaluates whether the learned mapping generalizes (calibration transfer from OOF to test).

---

##### `test_error_reliability_bins.csv`

**Represents:** reliability of the **aER estimates as probabilities**.

Instead of binning by confidence, this bins by **predicted error probability** and compares:

- mean predicted error in bin vs
- empirical error frequency in bin.

**Generated by:** `base_models_writer._write_test_error_metrics(...)`.

**Purpose:** this is the most direct calibration check for aER itself.

---

##### `coverage_accuracy_curve.csv` (best model only)

**Represents:** the computed risk–coverage curve.

Typical columns:

- `coverage`: fraction of accepted predictions at a given threshold
- `accuracy`: accuracy among accepted predictions
- (often) threshold values used along the curve

**Generated by:** sorting samples by aER (low risk first) and sweeping coverage.

**Purpose:** quantifies the selective prediction tradeoff.

---

##### `coverage_summary.csv` (best model only)

**Represents:** a condensed set of operating points (selected coverages and their accuracies / risks).

**Generated by:** summarizing `coverage_accuracy_curve.csv`.

**Purpose:** quick, reviewer-friendly summary for reports and appendices.

---

##### `clinician_view.csv` (best model only)

**Represents:** a clinician-facing per-sample table designed for manual audit.

Contains (at minimum):

- sample identifier (`row_id`)
- true label (if available in evaluation)
- predicted label
- predicted label in original string form (`y_pred_label`)
- aER as a probability and percent (`aer`, `aer_pct`)
- a flag if the predicted error exceeds the target (`flag_gt_target_error`)

**Generated by:** the best-model extras stage, using label decoding from prepared data.

**Purpose:** supports the thesis motivation: *per-sample risk communication* and identification of predictions that should trigger review.

---

##### `top20_highest_adaptive_error.csv` (best model only)

**Represents:** the 20 highest-risk predictions on the test set (by aER).

**Generated by:** sorting test per-sample outputs by `aer` descending.

**Purpose:** a compact “worst-case” audit list for debugging and case studies.

---

### C) `.../plots/`

##### `confidence_vs_expected_error.png`

**Represents:** a single-model plot of confidence vs expected error.

**Generated by:** plotting the binned OOF mapping (and sometimes test points) for that model.

**Purpose:** visual sanity check:
- should generally decrease (higher confidence → lower expected error),
- should be smooth/stable if bins are well-supported.

---

##### `coverage_vs_accuracy.png` (best model only)

**Represents:** the risk–coverage curve as a plot.

**Generated by:** plotting `coverage_accuracy_curve.csv`.

**Purpose:** communicates selective prediction behavior in one figure suitable for academic reports.

---

### D) `.../predictions/`

##### `oof_per_sample.parquet` (+ `.csv`)

**Represents:** per-sample OOF predictions used for calibration.

Typically includes:

- `row_id`, `fold`
- `y_true`, `y_pred_oof`
- candidate confidence metrics
- calibrated probabilities (parquet)
- `aer` and `aer_cv` (when available)

**Generated by:** OOF + fit stages.

**Purpose:** provides the raw calibration evidence and enables auditing the mapping construction.

---

##### `test_per_sample.parquet` (+ `.csv`)

**Represents:** per-sample test predictions with confidence diagnostics and aER.

Includes:

- `row_id`
- `y_true`, `y_pred`, `correct`
- `confidence`
- `aer`, `aer_pct`
- `flag_gt_target_error`
- decoded labels (`y_true_label`, `y_pred_label`)
- `y_pred_from_cal_proba`: the class implied by the **calibrated** probabilities (useful for sanity checks; calibration should not usually change the argmax class)
- probability diagnostics computed from calibrated probabilities:
  - `p_max`: max class probability
  - `p_2nd`: second-highest class probability
  - `p_margin`: `p_max - p_2nd`
  - `p_pred`: probability of the predicted class
  - `p_pred_margin`: margin relative to the predicted class

**Generated by:** test stage.

**Purpose:** the primary artifact for per-sample reliability analysis and downstream selective decision rules.

---

### E) `.../reports/`

These are Markdown summaries intended for humans.

##### `clinician_view.md` (best model only)
Markdown version of `clinician_view.csv` for quick reading.

##### `coverage_summary.md` (best model only)
Markdown report summarizing the risk–coverage operating points.

##### `risk_control_threshold.md` (best model only)
Human-readable explanation of the selected threshold \(t^*\).

---

#### 3.5.4 Ensemble outputs (optional): `adaptive_error/ensemble/`

Ensemble analysis is enabled with `--aer-ensemble`.

df-analyze writes:

##### `adaptive_error/ensemble/tables/ensemble_summary.csv`
A single table comparing all ensemble strategies on key metrics (accuracy, risk, coverage, calibration metrics).

##### `adaptive_error/ensemble/reports/ensemble_summary.md`
Markdown version of the same summary.

##### `adaptive_error/ensemble/<strategy_name>/...`
A per-strategy directory that mirrors the per-model structure:

- `metadata/strategy.json`: the strategy definition and parameters
- `metadata/hens_calibrator.json`: calibrator metadata where applicable
- `predictions/cv_ensemble.parquet`: cross-validated ensemble outputs used for risk calibration
- `predictions/test_per_sample.*`: per-sample test predictions and aER for the ensemble
- the same reliability tables and plots as base models

**Scientific purpose:** these artifacts allow direct evaluation of thesis-motivated ensemble ideas:
- whether risk-aware model selection/weighting improves selective prediction,
- whether the ensemble’s risk estimates remain calibrated.

---

## 4. Multi-target learning

### 4.1 How multi-target is integrated into df-analyze

Multi-target support was integrated by extending df-analyze’s **existing abstractions** rather than introducing a parallel pipeline.

At a high level, df-analyze makes these changes:

1. **CLI accepts multiple targets** (`--targets`)
2. **PreparedData stores y as a DataFrame** when multiple targets are provided
3. **Single-target components are reused via slicing** (`PreparedData.for_target(...)`)
4. **Feature selection is run per target**, then aggregated into a shared feature set
5. **Model training and evaluation accept multi-output y**, with consistent scoring and reporting

The key design principle is:  
> Multi-target is a natural extension of the existing pipeline: the same stages run, just repeated per target where the stage is inherently target-dependent.

#### Step-by-step integration

##### Step 0 — CLI layer: `--targets` as an extension of `--target`

- `--target` continues to exist for single-target runs (backwards compatibility).
- `--targets y1,y2,...` activates multi-target mode and overrides `--target`.

No new “multi-target runner” is introduced; it is the same `main()` entry point with a list of targets.

---

##### Step 1 — Preparation stage: multi-target y representation

In multi-target mode:

- `PreparedData.y` is a **DataFrame** with one column per target.
- For classification, `labels.json` becomes a **nested mapping**:
  - `{target_name: {encoded_int: original_label_string}}`

This allows:

- the same preprocessing logic to run,
- the same decoded-label reporting to remain correct per target.

---

##### Step 2 — Train/test splitting: stratification across multiple targets

For classification, df-analyze must preserve class balance.

Multi-target introduces a practical issue: the joint combination of target levels can explode, producing many rare combinations.

df-analyze solves this by constructing a **combined stratification label** from multiple targets, and (if necessary) dropping the most unique target(s) until the split is feasible.

This preserves the spirit of stratification without requiring a parallel sampling system.

---

##### Step 3 — Univariate analysis and feature selection: reuse by slicing

Most of df-analyze’s univariate association and prediction tools are inherently single-target.

Instead of rewriting them, df-analyze reuses them by:

1. iterating over targets,
2. calling `PreparedData.for_target(target)` to create a single-target view,
3. calling the existing univariate/selection routines unchanged,
4. writing outputs under per-target subdirectories.

This is the key “natural extension” mechanism: the same code is reused, just applied target-by-target.

---

##### Step 4 — Aggregate selected features across targets

After per-target selection, df-analyze aggregates selected features into one shared feature set.

This aggregated set is then passed into the standard tuning/evaluation stage.

Aggregation is configurable (see §4.2).

---

##### Step 5 — Model tuning and evaluation with multi-output y

df-analyze extends the existing model interface so models can accept:

- `y_train` as a Series (single-target) **or**
- `y_train` as a DataFrame (multi-target).

For tuning:

- a single Optuna trial produces a *single* parameter set,
- the trial objective is computed as the **mean score across targets**.

Runtime control (important in multi-target runs):

- df-analyze reuses your global `--htune-trials` budget but scales it *per target* so multi-target does not explode runtime.
- Concretely, the effective trials per run are approximately `max(15, htune_trials // n_targets)` (and CatBoost is additionally capped for stability).
- This keeps the total search effort roughly comparable to a single-target run while still exploring a meaningful hyperparameter space.

For reporting:

- df-analyze stores per-target rows in the long performance table (`target` column present),
- markdown output naturally expands to multiple targets.

No separate tuning system is created.

---

### 4.2 Multi-target feature selection aggregation

Multi-target selection is a two-level process:

1. Run selection per target (unchanged methods).
2. Combine (“aggregate”) the per-target outputs.

Two aggregation strategies are supported:

#### A) `--mt-agg-strategy borda` (default)

A rank aggregation approach:

- each target provides a ranked feature list,
- ranks are combined across targets,
- features that appear across more targets are favored via a support weighting exponent `--mt-agg-alpha`.

This is useful when each target is noisy but you want a stable shared feature subset.

#### B) `--mt-agg-strategy freq`

A frequency-based approach:

- count how often each feature was selected across targets,
- keep features that appear frequently.

This is simpler and ignores per-target ranking scores.

#### Support and size controls

- `--mt-min-support`: minimum fraction of targets that must include a feature to keep it
- `--mt-top-k`: maximum number of aggregated features to keep

These controls help keep multi-target models tractable and improve interpretability.

---

### 4.3 Multi-target models

df-analyze supports multi-target training for:

- **KNN** (native multi-output support via scikit-learn)
- **Dummy** (implemented as a per-target model bank)
- **CatBoost** (implemented as a per-target model bank)

This section explains how each was extended while preserving df-analyze’s existing model abstraction.

#### 4.3.1 Shared design: extend the base model interface, not the pipeline

The key architectural move is in `df_analyze/models/base.py`:

- `DfAnalyzeModel.fit(...)` now accepts `y` as Series **or** DataFrame.
- `predict(...)` returns:
  - a Series for single-target, or
  - a DataFrame with one column per target for multi-target.
- `predict_proba(...)` returns:
  - a probability array for single-target, or
  - a dict `{target_name: proba_array}` for multi-target where the underlying estimator returns a list of per-target arrays.

This means the tuning/evaluation pipeline does not need new branches: it calls the same methods, and the model wrapper normalizes outputs.

---

#### 4.3.2 KNN (multi-target)

**Where:** `df_analyze/models/knn.py`

**How multi-target support works:**

- scikit-learn’s KNN estimators accept multi-output `y` directly.
- df-analyze passes the multi-target DataFrame `y_train` into `.fit(...)`.
- `predict(...)` returns an \(n \times T\) array, which df-analyze wraps into a DataFrame with target column names.
- `predict_proba(...)` in scikit-learn multi-output classification returns a **list of probability arrays** (one per target). df-analyze converts this into a dict keyed by target name.

**Why this is a natural extension:**  
No separate KNN multi-target training loop is introduced; df-analyze uses the estimator’s native capability and adapts only the output formatting to match existing df-analyze expectations.

---

#### 4.3.3 Dummy (multi-target)

**Where:** `df_analyze/models/dummy.py`

**Why a wrapper is needed:**  
scikit-learn’s Dummy models are simplest for single-output tasks; multi-output support is not uniform across variants and does not provide consistent per-target probability interfaces.

**Implementation pattern:**

- df-analyze creates one Dummy estimator per target:
  - `self.models[target] = DummyClassifier(...)` or `DummyRegressor(...)`
- `.fit(...)` loops over targets and fits each model to its target column.
- `.predict(...)` loops over targets and concatenates predictions into a DataFrame.
- `.predict_proba(...)` returns a dict `{target: proba}`.

**Why this is scientifically reasonable:**  
Dummy is a baseline. Treating each target independently preserves the baseline interpretation and avoids introducing implicit coupling.

---

#### 4.3.4 CatBoost (multi-target)

**Where:** `df_analyze/models/catboost.py`

**Why a wrapper is needed:**  
CatBoost does not provide a single “multi-output classifier” interface compatible with df-analyze’s expected tuning and evaluation flow.

**Implementation pattern:**

- Similar to Dummy, df-analyze trains one CatBoost model per target:
  - `self.models[target] = CatBoostClassifier(...)` or `CatBoostRegressor(...)`
- This preserves:
  - target-wise hyperparameter control,
  - consistent probability outputs per target,
  - compatibility with df-analyze scoring/evaluation.

**Handling categorical features:**

- df-analyze uses CatBoost’s native categorical handling.
- Feature indices for categorical columns are derived from the prepared data’s `X_cat` / `X` structure.

**Compute considerations:**

- CatBoost can use GPU if available; df-analyze includes safe fallback behavior to CPU when GPU is not available or fails.
- Multi-target tuning can be computationally expensive; df-analyze therefore limits multi-target to a small model set and scales down trials per target.

---

## 5. Output changes in multi-target mode

Multi-target mode preserves df-analyze’s directory layout, but introduces two systematic differences:

1. **Target-dependent stages write into per-target subdirectories.**
2. **Final evaluation tables include a `target` column.**

### 5.1 Per-target subdirectories

When `--targets` is used, df-analyze writes target-dependent artifacts under:

- `features/associations/<target>/...`
- `features/predictions/<target>/...`
- `selection/<target>/...` (for per-target selection reports and data)

These contain the same kinds of files as the single-target pipeline, but repeated once per target.

### 5.2 Aggregated (shared) selection artifacts

After aggregating per-target selections, df-analyze writes the aggregated selection at the usual root locations:

- `selection/filter/...` (aggregated filter results)
- `selection/embed/...` and `selection/wrapper/...` (aggregated model-selection results)

The aggregated JSON payloads include `target_names` metadata so you can see which targets contributed.

### 5.3 Prepared data artifacts that change meaning

In `prepared/` (and the train/test prepared subfolders):

- `y.parquet`, `y_train.parquet`, `y_test.parquet` contain **multiple columns** (one per target).
- `labels.json` becomes nested (per target) for classification.

Everything else remains structurally identical.

### 5.4 Results tables become target-aware

In `results/`:

- `performance_long_table.csv` gains a `target` column.
- The Markdown summary naturally expands to multiple targets (each model/selection row can appear multiple times, once per target).

This is critical for scientific review: multi-target tuning uses averaged objectives, but reporting remains transparent per target.

---

## 6. Developer map: where the integrations live

If you are maintaining or extending these features, the key modules are:

### Adaptive Error Rate (aER)

- `df_analyze/analysis/adaptive_error/runner.py`  
  Orchestrates aER execution from prepared data and tuned results.
- `df_analyze/analysis/adaptive_error/oof.py` + `oof_stage.py`  
  Builds OOF predictions, selects probability calibrator and confidence metric.
- `df_analyze/analysis/adaptive_error/aer.py`  
  `AdaptiveErrorCalculator`: fits the confidence→expected error mapping.
- `df_analyze/analysis/adaptive_error/test_stage.py`  
  Applies mapping on test data, writes per-sample outputs and diagnostics.
- `df_analyze/analysis/adaptive_error/risk_control.py` + `risk_control_writer.py`  
  Risk-controlled threshold selection and reporting.
- `df_analyze/analysis/adaptive_error/ensemble_*`  
  Optional thesis-aligned ensemble extensions.

### Multi-target learning

- `df_analyze/cli/cli.py`  
  Adds `--targets` and multi-target aggregation options; maps them into `ProgramOptions`.
- `df_analyze/preprocessing/prepare.py`  
  Stores multi-target `y` as a DataFrame; adds `PreparedData.for_target(...)`.
- `df_analyze/splitting.py`  
  Implements multi-target-aware stratification labels (`y_split_label`).
- `df_analyze/selection/multitarget.py`  
  Implements aggregation of per-target selection results.
- `df_analyze/models/base.py`  
  Extends the model interface to accept/return multi-target structures.
- `df_analyze/models/knn.py`, `models/dummy.py`, `models/catboost.py`  
  Multi-target model implementations.

### Integration wiring

- `df_analyze/_main.py`  
  Executes per-target univariate/selection via `PreparedData.for_target(...)`, aggregates selections, runs multi-target tuning, and (if enabled) runs aER per target by slicing evaluation results.

---

## Appendix: How to read the aER outputs scientifically

If you only read one thing:

1. Start with `adaptive_error/tables/aer_metrics_by_model.csv` to identify promising models.
2. For the best model:
   - check `models/<slug>/plots/confidence_vs_expected_error.png` for sanity,
   - use `models/<slug>/tables/test_error_reliability_bins.csv` to judge whether aER values are calibrated,
   - use `models/<slug>/plots/coverage_vs_accuracy.png` to evaluate selective prediction tradeoffs,
   - use `models/<slug>/reports/risk_control_threshold.md` to understand the operational threshold.
3. Use `clinician_view.csv` and `top20_highest_adaptive_error.csv` for case-level audit.

This workflow matches the research intent in the thesis topic document: turn raw model confidence into interpretable, reviewable, risk-aware decision support.
